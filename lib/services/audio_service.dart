// lib/services/audio_service.dart
import 'dart:async';
import 'dart:io';
import 'package:logger/logger.dart';
import 'package:path_provider/path_provider.dart';
import 'package:record_platform_interface/record_platform_interface.dart';
import 'package:just_audio/just_audio.dart';
import 'package:audio_session/audio_session.dart';
import 'package:share_plus/share_plus.dart';

class AudioService {
  // Use the platform interface directly ‚Äî it exposes the create/start/stop API
  final String _recorderId = 'voicevibe_recorder';
  final RecordPlatform _record = RecordPlatform.instance;
  final Logger _logger = Logger();
  final AudioPlayer _audioPlayer = AudioPlayer();
  AudioSession? _audioSession;

  String? _currentPath;
  
  final StreamController<Duration> _durationController = StreamController<Duration>.broadcast();
  Stream<Duration> get onDurationChanged => _durationController.stream;
  Timer? _durationTimer;

  // Playback streams
  final StreamController<Duration> _playbackPositionController = StreamController<Duration>.broadcast();
  Stream<Duration> get onPlaybackPositionChanged => _playbackPositionController.stream;

  final StreamController<Duration> _playbackDurationController = StreamController<Duration>.broadcast();
  Stream<Duration> get onPlaybackDurationChanged => _playbackDurationController.stream;

  // Expose playing state stream from audio player
  Stream<bool> get onPlayingChanged => _audioPlayer.playingStream;

  final StreamController<void> _playbackCompleteController = StreamController<void>.broadcast();
  Stream<void> get onPlaybackComplete => _playbackCompleteController.stream;

  bool isRecording = false;

  Future<void> init() async {
    _logger.i('AudioService initialized with `record_platform_interface`.');
    try {
      await _record.create(_recorderId);
    } catch (e) {
      _logger.w('RecordPlatform.create() failed: $e');
    }

    // Configure audio session for correct behavior with other apps and background
    try {
      _audioSession = await AudioSession.instance;

      // Listen for interruptions (incoming calls, etc.) and pause/resume playback.
      // Handle event as dynamic to avoid depending on exact event type shape.
        _audioSession?.interruptionEventStream.listen((event) async {
          // event shape may vary between versions; access dynamically and safely
          try {
            final dyn = event as dynamic;
            bool? begin;
            bool? end;
            try {
              begin = dyn.begin as bool?;
            } catch (_) {
              try {
                begin = dyn['begin'] as bool?;
              } catch (_) {}
            }
            try {
              end = dyn.end as bool?;
            } catch (_) {
              try {
                end = dyn['end'] as bool?;
              } catch (_) {}
            }
            if (begin == true) {
              await _audioPlayer.pause();
            }
            if (end == true) {
              await _audioPlayer.play();
            }
          } catch (_) {}
        });
    } catch (e) {
      _logger.w('–ù–µ —É–¥–∞–ª–æ—Å—å –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å AudioSession: $e');
    }

    // Forward just_audio position/duration streams to our controllers
    // just_audio streams are typed as Duration ‚Äî forward directly
    _audioPlayer.positionStream.listen((Duration pos) {
      try {
        // just_audio provides Duration-typed positions; forward directly
        _playbackPositionController.add(pos);
      } catch (_) {}
    });

    _audioPlayer.durationStream.listen((Duration? dur) {
      try {
        if (dur != null) _playbackDurationController.add(dur);
      } catch (_) {}
    });

    _audioPlayer.playerStateStream.listen((state) {
      if (state.processingState == ProcessingState.completed) {
        _playbackCompleteController.add(null);
      }
    });
  }

  Future<String?> startRecording() async {
    try {
      final hasPerm = await _record.hasPermission(_recorderId);
      if (hasPerm) {
        final dir = await getApplicationDocumentsDirectory();
        // –ó–∞–ø–∏—Å—ã–≤–∞–µ–º –≤ WAV/PCM16, 16kHz, –º–æ–Ω–æ ‚Äî —ç—Ç–æ —Ñ–æ—Ä–º–∞—Ç, –∫–æ—Ç–æ—Ä—ã–π –æ–∂–∏–¥–∞–µ—Ç Vosk
        _currentPath = '${dir.path}/note_${DateTime.now().millisecondsSinceEpoch}.wav';

        // Determine a supported encoder and fallback if needed
        AudioEncoder preferred = AudioEncoder.wav;
        final supportedWav = await _record.isEncoderSupported(_recorderId, AudioEncoder.wav);
        final supportedPcm = await _record.isEncoderSupported(_recorderId, AudioEncoder.pcm16bits);
        final supportedAac = await _record.isEncoderSupported(_recorderId, AudioEncoder.aacLc);

        AudioEncoder chosen;
        if (supportedWav) {
          chosen = AudioEncoder.wav;
        } else if (supportedPcm) {
          chosen = AudioEncoder.pcm16bits;
        } else if (supportedAac) {
          chosen = AudioEncoder.aacLc;
        } else {
          // If none supported, fallback to wav and let platform decide or fail
          chosen = preferred;
          _logger.w('–ù–µ—Ç –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã—Ö —ç–Ω–∫–æ–¥–µ—Ä–æ–≤ (wav/pcm/aac) ‚Äî –ø—ã—Ç–∞–µ–º—Å—è —Å wav.');
        }

        // adjust extension based on chosen encoder
        String ext = '.wav';
        switch (chosen) {
          case AudioEncoder.wav:
            ext = '.wav';
            break;
          case AudioEncoder.pcm16bits:
            ext = '.s16';
            break;
          case AudioEncoder.aacLc:
            ext = '.m4a';
            break;
          default:
            ext = '.wav';
        }

        _currentPath = '${dir.path}/note_${DateTime.now().millisecondsSinceEpoch}$ext';

        final config = RecordConfig(
          encoder: chosen,
          sampleRate: 16000,
          numChannels: 1,
          bitRate: 128000,
        );

        _logger.i('–í—ã–±—Ä–∞–Ω —ç–Ω–∫–æ–¥–µ—Ä: $chosen, –ø—É—Ç—å: $_currentPath');

        await _record.start(
          _recorderId,
          config,
          path: _currentPath!,
        );

        _logger.i('üé§ –ù–∞—á–∏–Ω–∞–µ–º –∑–∞–ø–∏—Å—å: $_currentPath');
        isRecording = true;
        _startDurationTimer();
        return _currentPath;
      } else {
        _logger.e('–ù–µ—Ç –ø—Ä–∞–≤ –Ω–∞ –∑–∞–ø–∏—Å—å –∞—É–¥–∏–æ');
        return null;
      }
    } catch (e) {
      _logger.e('–û—à–∏–±–∫–∞ –Ω–∞—á–∞–ª–∞ –∑–∞–ø–∏—Å–∏: $e');
      return null;
    }
  }

  void _startDurationTimer() {
    _durationTimer?.cancel();
    _durationTimer = Timer.periodic(const Duration(seconds: 1), (timer) {
      _durationController.add(Duration(seconds: timer.tick));
    });
  }

  Future<String?> stopRecording() async {
    _durationTimer?.cancel();
    _durationController.add(Duration.zero);
    try {
  final path = await _record.stop(_recorderId);
      _logger.i('‚úÖ –ó–∞–ø–∏—Å—å –æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞. –ü—É—Ç—å: $path');
      isRecording = false;
      return path;
    } catch (e) {
      _logger.e('–û—à–∏–±–∫–∞ –æ—Å—Ç–∞–Ω–æ–≤–∫–∏ –∑–∞–ø–∏—Å–∏: $e');
      return null;
    }
  }

  // –ò–ó–ú–ï–ù–ï–ù–ò–ï: –î–æ–±–∞–≤–ª–µ–Ω—ã –º–µ—Ç–æ–¥—ã –ø–∞—É–∑—ã –∏ –≤–æ–∑–æ–±–Ω–æ–≤–ª–µ–Ω–∏—è
  Future<void> pauseRecording() async {
    try {
      _durationTimer?.cancel();
  await _record.pause(_recorderId);
      _logger.i('–ü–∞—É–∑–∞ –∑–∞–ø–∏—Å–∏.');
    } catch (e) {
      _logger.e('–û—à–∏–±–∫–∞ –ø–∞—É–∑—ã: $e');
    }
  }

  Future<void> resumeRecording() async {
    try {
      _startDurationTimer();
  await _record.resume(_recorderId);
      _logger.i('–í–æ–∑–æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ –∑–∞–ø–∏—Å–∏.');
    } catch (e) {
      _logger.e('–û—à–∏–±–∫–∞ –≤–æ–∑–æ–±–Ω–æ–≤–ª–µ–Ω–∏—è: $e');
    }
  }
  
  // –§—É–Ω–∫—Ü–∏–∏ –Ω–∏–∂–µ –ø–æ–∫–∞ –Ω–µ –±—É–¥—É—Ç —Ä–∞–±–æ—Ç–∞—Ç—å, —Ç–∞–∫ –∫–∞–∫ –¥–ª—è –ø—Ä–æ–∏–≥—Ä—ã–≤–∞–Ω–∏—è –Ω—É–∂–µ–Ω –¥—Ä—É–≥–æ–π –ø–∞–∫–µ—Ç
  Future<void> playNote(String path) async {
    try {
      if (path.isEmpty) return;
      final file = File(path);
      if (!await file.exists()) {
        _logger.w('–§–∞–π–ª –¥–ª—è –ø—Ä–æ–∏–≥—Ä—ã–≤–∞–Ω–∏—è –Ω–µ –Ω–∞–π–¥–µ–Ω: $path');
        return;
      }
      _logger.i('–ü—Ä–æ–∏–≥—Ä—ã–≤–∞–µ–º –∑–∞–º–µ—Ç–∫—É: $path');
      // Activate audio session before playback so system knows our intent
      try {
        await _audioSession?.setActive(true);
      } catch (e) {
        _logger.w('–ù–µ —É–¥–∞–ª–æ—Å—å —É—Å—Ç–∞–Ω–æ–≤–∏—Ç—å —Å–µ—Å—Å–∏—é –∞–∫—Ç–∏–≤–Ω–æ–π: $e');
      }

      await _audioPlayer.setFilePath(path);
      await _audioPlayer.play();
    } catch (e) {
      _logger.e('–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø—Ä–æ–∏–≥—Ä—ã–≤–∞–Ω–∏–∏ –∑–∞–º–µ—Ç–∫–∏: $e');
    }
  }

  Future<void> stopPlayer() async {
    try {
      await _audioPlayer.stop();
      try {
        await _audioSession?.setActive(false);
      } catch (_) {}
    } catch (e) {
      _logger.e('–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ—Å—Ç–∞–Ω–æ–≤–∫–µ –ø–ª–µ–µ—Ä–∞: $e');
    }
  }

  Future<void> pausePlayer() async {
    try {
      await _audioPlayer.pause();
    } catch (e) {
      _logger.e('–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–∞—É–∑–µ –ø–ª–µ–µ—Ä–∞: $e');
    }
  }

  Future<void> resumePlayer() async {
    try {
      await _audioPlayer.play();
    } catch (e) {
      _logger.e('–û—à–∏–±–∫–∞ –ø—Ä–∏ –≤–æ–∑–æ–±–Ω–æ–≤–ª–µ–Ω–∏–∏ –ø–ª–µ–µ—Ä–∞: $e');
    }
  }

  bool get isPlaying {
    return _audioPlayer.playing;
  }
  
  Future<void> dispose() async {
    try {
  await _record.dispose(_recorderId);
    } catch (_) {
      // Some platform implementations may not require/implement dispose.
    }
    try {
      await _audioPlayer.dispose();
    } catch (_) {}
    _durationController.close();
    _playbackPositionController.close();
    _playbackDurationController.close();
    _playbackCompleteController.close();
    _durationTimer?.cancel();
    _logger.i('AudioService disposed.');
  }

  Future<void> saveText(String audioPath, String title, String text) async {
    final textPath = audioPath.replaceAll(RegExp(r'\.\w+$'), '.txt');
    final file = File(textPath);
    // –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ —Ñ–æ—Ä–º–∞—Ç–µ: –ø–µ—Ä–≤–∞—è —Å—Ç—Ä–æ–∫–∞ - –∑–∞–≥–æ–ª–æ–≤–æ–∫, –æ—Å—Ç–∞–ª—å–Ω—ã–µ - —Ç–µ–∫—Å—Ç
    await file.writeAsString('$title\n$text');
  }

  Future<void> deleteNote(String audioPath) async {
    try {
      final audioFile = File(audioPath);
      if (await audioFile.exists()) {
        await audioFile.delete();
      }
      // –£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–æ –∑–∞–º–µ–Ω—è–µ–º –ª—é–±–æ–µ –∞—É–¥–∏–æ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ –Ω–∞ .txt
      final textPath = audioPath.replaceAll(RegExp(r'\.\w+$'), '.txt');
      final textFile = File(textPath);
      if (await textFile.exists()) {
        await textFile.delete();
      }
    } catch (e) {
      _logger.e('–û—à–∏–±–∫–∞ —É–¥–∞–ª–µ–Ω–∏—è —Ñ–∞–π–ª–∞ –∑–∞–º–µ—Ç–∫–∏: $e');
    }
  }
  
  // –ò–ó–ú–ï–ù–ï–ù–û: –ú–µ—Ç–æ–¥ —Ç–µ–ø–µ—Ä—å –ø—Ä–∏–Ω–∏–º–∞–µ—Ç –∑–∞–≥–æ–ª–æ–≤–æ–∫
  Future<void> exportNote(String path, String title, String text) async {
    try {
      final audioFile = XFile(path);
      // –ò—Å–ø–æ–ª—å–∑—É–µ–º –∑–∞–≥–æ–ª–æ–≤–æ–∫ –∑–∞–º–µ—Ç–∫–∏ –∫–∞–∫ —Ç–µ–º—É –ø–∏—Å—å–º–∞
      final subject = title;

      if (text.isNotEmpty) {
        await Share.shareXFiles(
          [audioFile],
          text: '–¢–µ–∫—Å—Ç –∑–∞–º–µ—Ç–∫–∏: \n\n"$text"',
          subject: subject,
        );
      } else {
        await Share.shareXFiles(
          [audioFile],
          subject: subject,
        );
      }
      _logger.i('–ó–∞–º–µ—Ç–∫–∞ "$path" —É—Å–ø–µ—à–Ω–æ —ç–∫—Å–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–∞.');
    } catch (e) {
      _logger.e('–û—à–∏–±–∫–∞ —ç–∫—Å–ø–æ—Ä—Ç–∞ –∑–∞–º–µ—Ç–∫–∏: $e');
    }
  }
}